{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.14)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from google.cloud import storage\n",
    "\n",
    "from dataset import FontDataset, CollateFN\n",
    "from model import FontDiffuserModel\n",
    "from criterion import ContentPerceptualLoss\n",
    "from build import build_unet, build_style_encoder, build_content_encoder, build_ddpm_scheduler\n",
    "from args import TrainPhase1Args\n",
    "from utils import x0_from_epsilon, reNormalize_img, normalize_mean_std, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the down block  DownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  DownBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n"
     ]
    }
   ],
   "source": [
    "args = TrainPhase1Args()\n",
    "unet = build_unet(args=args)\n",
    "style_encoder = build_style_encoder(args=args)\n",
    "content_encoder = build_content_encoder(args=args)\n",
    "noise_scheduler = build_ddpm_scheduler(args)\n",
    "storage_client = storage.Client(args.bucket_name)\n",
    "bucket = storage_client.bucket(args.bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FontDiffuserModel(\n",
    "    unet=unet,\n",
    "    style_encoder=style_encoder,\n",
    "    content_encoder=content_encoder)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptual_loss = ContentPerceptualLoss()\n",
    "train_font_dataset = FontDataset(args)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_font_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=args.train_batch_size,\n",
    "    collate_fn=CollateFN())\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201c5bd33d4f4723ba582d5e3f5c7a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/jupyter/ai_font/exp0514/diffuser_van_phase1/model.py:34: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  style_img_feature, _, _ = self.style_encoder(style_images)\n",
      "/home/jupyter/ai_font/exp0514/diffuser_van_phase1/model.py:40: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  content_img_feature, content_residual_features = self.content_encoder(content_images)\n",
      "/home/jupyter/ai_font/exp0514/diffuser_van_phase1/model.py:43: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  style_content_feature, style_content_res_features = self.content_encoder(style_images)\n",
      "/home/jupyter/ai_font/exp0514/diffuser_van_phase1/model.py:49: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.unet'.\n",
      "  out = self.unet(\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(args.max_train_steps))\n",
    "progress_bar.set_description(\"Steps\")\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "global_step = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, samples in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        content_images = samples[\"content_image\"].cuda()\n",
    "        style_images = samples[\"style_image\"].cuda()\n",
    "        target_images = samples[\"target_image\"].cuda()\n",
    "        nonorm_target_images = samples[\"nonorm_target_image\"].cuda()\n",
    "\n",
    "        noise = torch.randn_like(target_images)\n",
    "        bsz = target_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the target_images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "\n",
    "        # Classifier-free training strategy\n",
    "        context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "        for i, mask_value in enumerate(context_mask):\n",
    "            if mask_value==1:\n",
    "                content_images[i, :, :, :] = 1\n",
    "                style_images[i, :, :, :] = 1\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        noise_pred, offset_out_sum = model(\n",
    "            x_t=noisy_target_images,\n",
    "            timesteps=timesteps,\n",
    "            style_images=style_images,\n",
    "            content_images=content_images,\n",
    "            content_encoder_downsample_size=args.content_encoder_downsample_size)\n",
    "        diff_loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "        offset_loss = offset_out_sum / 2\n",
    "\n",
    "        # output processing for content perceptual loss\n",
    "        pred_original_sample_norm = x0_from_epsilon(\n",
    "            scheduler=noise_scheduler,\n",
    "            noise_pred=noise_pred,\n",
    "            x_t=noisy_target_images,\n",
    "            timesteps=timesteps)\n",
    "        pred_original_sample = reNormalize_img(pred_original_sample_norm)\n",
    "        norm_pred_ori = normalize_mean_std(pred_original_sample)\n",
    "        norm_target_ori = normalize_mean_std(nonorm_target_images)\n",
    "        percep_loss = perceptual_loss.calculate_loss(\n",
    "            generated_images=norm_pred_ori,\n",
    "            target_images=norm_target_ori,\n",
    "            device=target_images.device)\n",
    "\n",
    "        loss = diff_loss + \\\n",
    "                args.perceptual_coefficient * percep_loss + \\\n",
    "                    args.offset_coefficient * offset_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if global_step % 10000 == 0:\n",
    "            save_model(args, bucket, unet.state_dict(), \"unet_%s\"%str(global_step))\n",
    "            save_model(args, bucket, style_encoder.state_dict(), \"style_encoder_%s\"%str(global_step))\n",
    "            save_model(args, bucket, content_encoder.state_dict(), \"content_encoder_%s\"%str(global_step))\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Quit\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_pred_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_target_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
