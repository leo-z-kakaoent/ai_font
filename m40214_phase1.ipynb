{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6138f845-9579-406c-896e-e7a38fa03ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f26e737-9ef4-4a38-9ffa-d31076718d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_scheduler\n",
    "from font_diffuser.dataset import FontDataset, CollateFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3c9c55e-19cf-4eaa-98a3-81d7a7ca406c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from font_diffuser.model import FontDiffuserModel\n",
    "from font_diffuser.criterion import ContentPerceptualLoss\n",
    "from font_diffuser.build import build_unet, build_style_encoder, build_content_encoder, build_ddpm_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d9c86-c2a7-4731-b611-6d37f456df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import (save_args_to_yaml,\n",
    "                   x0_from_epsilon, \n",
    "                   reNormalize_img, \n",
    "                   normalize_mean_std)\n",
    "\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "def get_args():\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "    style_image_size = args.style_image_size\n",
    "    content_image_size = args.content_image_size\n",
    "    args.style_image_size = (style_image_size, style_image_size)\n",
    "    args.content_image_size = (content_image_size, content_image_size)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = get_args()\n",
    "\n",
    "    logging_dir = f\"{args.output_dir}/{args.logging_dir}\"\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        project_dir=logging_dir)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        filename=f\"{args.output_dir}/fontdiffuser_training.log\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO)\n",
    "\n",
    "    # Ser training seed\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    # Load model and noise_scheduler\n",
    "    unet = build_unet(args=args)\n",
    "    style_encoder = build_style_encoder(args=args)\n",
    "    content_encoder = build_content_encoder(args=args)\n",
    "    noise_scheduler = build_ddpm_scheduler(args)\n",
    "\n",
    "    model = FontDiffuserModel(\n",
    "        unet=unet,\n",
    "        style_encoder=style_encoder,\n",
    "        content_encoder=content_encoder)\n",
    "\n",
    "    # Build content perceptaual Loss\n",
    "    perceptual_loss = ContentPerceptualLoss()\n",
    "\n",
    "    # Load SCR module for supervision\n",
    "    if args.phase_2:\n",
    "        scr = build_scr(args=args)\n",
    "        scr.load_state_dict(torch.load(args.scr_ckpt_path))\n",
    "        scr.requires_grad_(False)\n",
    "\n",
    "    # Load the datasets\n",
    "    content_transforms = transforms.Compose(\n",
    "        [transforms.Resize(args.content_image_size, \n",
    "                           interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5], [0.5])])\n",
    "    style_transforms = transforms.Compose(\n",
    "        [transforms.Resize(args.style_image_size, \n",
    "                           interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5], [0.5])])\n",
    "    target_transforms = transforms.Compose(\n",
    "        [transforms.Resize((args.resolution, args.resolution), \n",
    "                           interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5], [0.5])])\n",
    "    train_font_dataset = FontDataset(\n",
    "        args=args,\n",
    "        phase='train', \n",
    "        transforms=[\n",
    "            content_transforms, \n",
    "            style_transforms, \n",
    "            target_transforms],\n",
    "        scr=args.phase_2)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_font_dataset, shuffle=True, batch_size=args.train_batch_size, collate_fn=CollateFN())\n",
    "    \n",
    "    # Build optimizer and learning rate\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,)\n",
    "\n",
    "    # Accelerate preparation\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler)\n",
    "    ## move scr module to the target deivces\n",
    "    if args.phase_2:\n",
    "        scr = scr.to(accelerator.device)\n",
    "\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(args.experience_name)\n",
    "        save_args_to_yaml(args=args, output_file=f\"{args.output_dir}/{args.experience_name}_config.yaml\")\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    # Convert to the training epoch\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        train_loss = 0.0\n",
    "        for step, samples in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            content_images = samples[\"content_image\"]\n",
    "            style_images = samples[\"style_image\"]\n",
    "            target_images = samples[\"target_image\"]\n",
    "            nonorm_target_images = samples[\"nonorm_target_image\"]\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Sample noise that we'll add to the samples\n",
    "                noise = torch.randn_like(target_images)\n",
    "                bsz = target_images.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the target_images according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "\n",
    "                # Classifier-free training strategy\n",
    "                context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "                for i, mask_value in enumerate(context_mask):\n",
    "                    if mask_value==1:\n",
    "                        content_images[i, :, :, :] = 1\n",
    "                        style_images[i, :, :, :] = 1\n",
    "\n",
    "                # Predict the noise residual and compute loss\n",
    "                noise_pred, offset_out_sum = model(\n",
    "                    x_t=noisy_target_images, \n",
    "                    timesteps=timesteps, \n",
    "                    style_images=style_images,\n",
    "                    content_images=content_images,\n",
    "                    content_encoder_downsample_size=args.content_encoder_downsample_size)\n",
    "                diff_loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                offset_loss = offset_out_sum / 2\n",
    "                \n",
    "                # output processing for content perceptual loss\n",
    "                pred_original_sample_norm = x0_from_epsilon(\n",
    "                    scheduler=noise_scheduler,\n",
    "                    noise_pred=noise_pred,\n",
    "                    x_t=noisy_target_images,\n",
    "                    timesteps=timesteps)\n",
    "                pred_original_sample = reNormalize_img(pred_original_sample_norm)\n",
    "                norm_pred_ori = normalize_mean_std(pred_original_sample)\n",
    "                norm_target_ori = normalize_mean_std(nonorm_target_images)\n",
    "                percep_loss = perceptual_loss.calculate_loss(\n",
    "                    generated_images=norm_pred_ori,\n",
    "                    target_images=norm_target_ori,\n",
    "                    device=target_images.device)\n",
    "                \n",
    "                loss = diff_loss + \\\n",
    "                        args.perceptual_coefficient * percep_loss + \\\n",
    "                            args.offset_coefficient * offset_loss\n",
    "                \n",
    "                if args.phase_2:\n",
    "                    neg_images = samples[\"neg_images\"]\n",
    "                    # sc loss\n",
    "                    sample_style_embeddings, pos_style_embeddings, neg_style_embeddings = scr(\n",
    "                        pred_original_sample_norm, \n",
    "                        target_images, \n",
    "                        neg_images, \n",
    "                        nce_layers=args.nce_layers)\n",
    "                    sc_loss = scr.calculate_nce_loss(\n",
    "                        sample_s=sample_style_embeddings,\n",
    "                        pos_s=pos_style_embeddings,\n",
    "                        neg_s=neg_style_embeddings)\n",
    "                    loss += args.sc_coefficient * sc_loss\n",
    "\n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if global_step % args.ckpt_interval == 0:\n",
    "                        save_dir = f\"{args.output_dir}/global_step_{global_step}\"\n",
    "                        os.makedirs(save_dir, exist_ok=True)\n",
    "                        torch.save(model.unet.state_dict(), f\"{save_dir}/unet.pth\")\n",
    "                        torch.save(model.style_encoder.state_dict(), f\"{save_dir}/style_encoder.pth\")\n",
    "                        torch.save(model.content_encoder.state_dict(), f\"{save_dir}/content_encoder.pth\")\n",
    "                        torch.save(model, f\"{save_dir}/total_model.pth\")\n",
    "                        logging.info(f\"[{time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))}] Save the checkpoint on global step {global_step}\")\n",
    "                        print(\"Save the checkpoint on global step {}\".format(global_step))\n",
    "\n",
    "            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            if global_step % args.log_interval == 0:\n",
    "                logging.info(f\"[{time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))}] Global Step {global_step} => train_loss = {loss}\")\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            \n",
    "            # Quit\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
