{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18eaba2-3d6e-42fc-b9c9-27cd0314278c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from font_diffuser.dataset import FontDataset, CollateFN\n",
    "\n",
    "from font_diffuser.model import FontDiffuserModel\n",
    "from font_diffuser.criterion import ContentPerceptualLoss\n",
    "from font_diffuser.build import build_unet, build_style_encoder, build_content_encoder, build_ddpm_scheduler\n",
    "from font_diffuser.args import TrainPhase1Args\n",
    "from font_diffuser.utils import save_args_to_yaml, x0_from_epsilon, reNormalize_img, normalize_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62d4f2d-ae2d-4866-82d3-cf7dd06380cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainPhase1Args()\n",
    "set_seed(args.seed)\n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ed0b14-4aec-4da0-9630-db7b6247fb71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the down block  DownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  DownBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n"
     ]
    }
   ],
   "source": [
    "unet = build_unet(args=args)\n",
    "style_encoder = build_style_encoder(args=args)\n",
    "content_encoder = build_content_encoder(args=args)\n",
    "noise_scheduler = build_ddpm_scheduler(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febf7336-e797-46f8-8073-d3d13846bcf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FontDiffuserModel(\n",
    "    unet=unet,\n",
    "    style_encoder=style_encoder,\n",
    "    content_encoder=content_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8271dbb2-33b0-46db-a671-26f7bf9ca76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptual_loss = ContentPerceptualLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de183401-84d4-4b75-ad9d-d1ead3aa393b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_font_dataset = FontDataset(path=args.path, phase='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_font_dataset, \n",
    "    shuffle=True, \n",
    "    batch_size=args.train_batch_size, \n",
    "    collate_fn=CollateFN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b241e1b-5942-4296-83e5-a0ef78305e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd2c687-444a-4f2c-b59b-be44684c2e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step, samples = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fcf6cc7-15ec-48e4-bc4a-f5270bc49da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8992df78-bfc8-4460-ac72-2819b340f526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "content_images = samples[\"content_image\"].cuda()\n",
    "style_images = samples[\"style_image\"].cuda()\n",
    "target_images = samples[\"target_image\"].cuda()\n",
    "nonorm_target_images = samples[\"nonorm_target_image\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d91ea38-0d68-4b52-93c9-189728349b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = style_images[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978c88bb-6e5e-4f35-a296-63b666971d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = style_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade8a7a-7e06-427e-8052-8c1770c4b395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672501c8-d1f1-48de-9a89-4464070d68b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn_like(target_images)\n",
    "bsz = target_images.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "timesteps = timesteps.long()\n",
    "\n",
    "# Add noise to the target_images according to the noise magnitude at each timestep\n",
    "# (this is the forward diffusion process)\n",
    "noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "for i, mask_value in enumerate(context_mask):\n",
    "    if mask_value==1:\n",
    "        content_images[i, :, :, :] = 1\n",
    "        style_images[i, :, :, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c6eab-71ec-4609-abe1-c9971bb2f9be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_pred, offset_out_sum = model(\n",
    "                x_t=noisy_target_images, \n",
    "                timesteps=timesteps, \n",
    "                style_images=style_images,\n",
    "                content_images=content_images,\n",
    "                content_encoder_downsample_size=args.content_encoder_downsample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e389c0-50d1-4f33-a5f6-5e0ceb94fac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fc5bc-8eb0-4460-a3d8-0054f00ffa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021da59d-a6d3-46e1-9e94-f9d6bfc979f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709d9f5c-7165-48b7-9374-ae90f11bded0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step, samples = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21179b40-b646-4fd7-8dfe-4decf20dcb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce06a376de6c49c6ac980c246176ee12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/jupyter/ai_font/font_diffuser/model.py:34: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  style_img_feature, _, _ = self.style_encoder(style_images)\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "global_step = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    train_loss = 0.0\n",
    "    for step, samples in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        content_images = samples[\"content_image\"]\n",
    "        style_images = samples[\"style_image\"]\n",
    "        target_images = samples[\"target_image\"]\n",
    "        nonorm_target_images = samples[\"nonorm_target_image\"]\n",
    "\n",
    "        print(\"a\")\n",
    "        with accelerator.accumulate(model):\n",
    "            # Sample noise that we'll add to the samples\n",
    "            noise = torch.randn_like(target_images)\n",
    "            bsz = target_images.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the target_images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "\n",
    "            # Classifier-free training strategy\n",
    "            context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "            for i, mask_value in enumerate(context_mask):\n",
    "                if mask_value==1:\n",
    "                    content_images[i, :, :, :] = 1\n",
    "                    style_images[i, :, :, :] = 1\n",
    "\n",
    "            print(\"c\")\n",
    "            # Predict the noise residual and compute loss\n",
    "            noise_pred, offset_out_sum = model(\n",
    "                x_t=noisy_target_images, \n",
    "                timesteps=timesteps, \n",
    "                style_images=style_images,\n",
    "                content_images=content_images,\n",
    "                content_encoder_downsample_size=args.content_encoder_downsample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaa426-ce0d-4b74-a244-80b22e1fdaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ffa3b43a4c417d8ce8232b3f474574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/jupyter/ai_font/font_diffuser/model.py:34: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  style_img_feature, _, _ = self.style_encoder(style_images)\n"
     ]
    }
   ],
   "source": [
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(args.experience_name)\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "global_step = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    train_loss = 0.0\n",
    "    for step, samples in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        content_images = samples[\"content_image\"]\n",
    "        style_images = samples[\"style_image\"]\n",
    "        target_images = samples[\"target_image\"]\n",
    "        nonorm_target_images = samples[\"nonorm_target_image\"]\n",
    "\n",
    "        print(\"a\")\n",
    "        with accelerator.accumulate(model):\n",
    "            # Sample noise that we'll add to the samples\n",
    "            noise = torch.randn_like(target_images)\n",
    "            bsz = target_images.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the target_images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "\n",
    "            # Classifier-free training strategy\n",
    "            context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "            for i, mask_value in enumerate(context_mask):\n",
    "                if mask_value==1:\n",
    "                    content_images[i, :, :, :] = 1\n",
    "                    style_images[i, :, :, :] = 1\n",
    "\n",
    "            print(\"c\")\n",
    "            # Predict the noise residual and compute loss\n",
    "            noise_pred, offset_out_sum = model(\n",
    "                x_t=noisy_target_images, \n",
    "                timesteps=timesteps, \n",
    "                style_images=style_images,\n",
    "                content_images=content_images,\n",
    "                content_encoder_downsample_size=args.content_encoder_downsample_size)\n",
    "            print(\"c1\")\n",
    "            diff_loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "            offset_loss = offset_out_sum / 2\n",
    "\n",
    "            print(\"d\")\n",
    "            # output processing for content perceptual loss\n",
    "            pred_original_sample_norm = x0_from_epsilon(\n",
    "                scheduler=noise_scheduler,\n",
    "                noise_pred=noise_pred,\n",
    "                x_t=noisy_target_images,\n",
    "                timesteps=timesteps)\n",
    "            pred_original_sample = reNormalize_img(pred_original_sample_norm)\n",
    "            norm_pred_ori = normalize_mean_std(pred_original_sample)\n",
    "            norm_target_ori = normalize_mean_std(nonorm_target_images)\n",
    "            percep_loss = perceptual_loss.calculate_loss(\n",
    "                generated_images=norm_pred_ori,\n",
    "                target_images=norm_target_ori,\n",
    "                device=target_images.device)\n",
    "\n",
    "            print(\"e\")\n",
    "            loss = diff_loss + \\\n",
    "                    args.perceptual_coefficient * percep_loss + \\\n",
    "                        args.offset_coefficient * offset_loss\n",
    "\n",
    "            print(\"f\")\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "            print(\"g\")\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(\"b\")\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            train_loss = 0.0\n",
    "\n",
    "        # Quit\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a20f3-eb86-4e2e-aa00-118d1c40bdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
