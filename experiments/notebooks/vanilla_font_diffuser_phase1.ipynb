{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from diffusers.optimization import get_scheduler\n",
    "from dataset import FontDataset, CollateFN\n",
    "\n",
    "from model import FontDiffuserModel\n",
    "from criterion import ContentPerceptualLoss\n",
    "from build import build_unet, build_style_encoder, build_content_encoder, build_ddpm_scheduler\n",
    "from args import TrainPhase1Args\n",
    "from utils import save_args_to_yaml, x0_from_epsilon, reNormalize_img, normalize_mean_std\n",
    "\n",
    "args = TrainPhase1Args()\n",
    "unet = build_unet(args=args)\n",
    "style_encoder = build_style_encoder(args=args)\n",
    "content_encoder = build_content_encoder(args=args)\n",
    "noise_scheduler = build_ddpm_scheduler(args)\n",
    "\n",
    "model = FontDiffuserModel(\n",
    "    unet=unet,\n",
    "    style_encoder=style_encoder,\n",
    "    content_encoder=content_encoder)\n",
    "\n",
    "perceptual_loss = ContentPerceptualLoss()\n",
    "\n",
    "train_font_dataset = FontDataset(path=args.path, phase='train')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_font_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=args.train_batch_size,\n",
    "    collate_fn=CollateFN())\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,)\n",
    "\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "global_step = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, samples in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        content_images = samples[\"content_image\"]\n",
    "        style_images = samples[\"style_image\"]\n",
    "        target_images = samples[\"target_image\"]\n",
    "        nonorm_target_images = samples[\"nonorm_target_image\"]\n",
    "\n",
    "        noise = torch.randn_like(target_images)\n",
    "        bsz = target_images.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=target_images.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the target_images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_target_images = noise_scheduler.add_noise(target_images, noise, timesteps)\n",
    "\n",
    "        # Classifier-free training strategy\n",
    "        context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)\n",
    "        for i, mask_value in enumerate(context_mask):\n",
    "            if mask_value==1:\n",
    "                content_images[i, :, :, :] = 1\n",
    "                style_images[i, :, :, :] = 1\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        noise_pred, offset_out_sum = model(\n",
    "            x_t=noisy_target_images,\n",
    "            timesteps=timesteps,\n",
    "            style_images=style_images,\n",
    "            content_images=content_images,\n",
    "            content_encoder_downsample_size=args.content_encoder_downsample_size)\n",
    "        diff_loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "        offset_loss = offset_out_sum / 2\n",
    "\n",
    "        # output processing for content perceptual loss\n",
    "        pred_original_sample_norm = x0_from_epsilon(\n",
    "            scheduler=noise_scheduler,\n",
    "            noise_pred=noise_pred,\n",
    "            x_t=noisy_target_images,\n",
    "            timesteps=timesteps)\n",
    "        pred_original_sample = reNormalize_img(pred_original_sample_norm)\n",
    "        norm_pred_ori = normalize_mean_std(pred_original_sample)\n",
    "        norm_target_ori = normalize_mean_std(nonorm_target_images)\n",
    "        percep_loss = perceptual_loss.calculate_loss(\n",
    "            generated_images=norm_pred_ori,\n",
    "            target_images=norm_target_ori,\n",
    "            device=target_images.device)\n",
    "\n",
    "        loss = diff_loss + \\\n",
    "                args.perceptual_coefficient * percep_loss + \\\n",
    "                    args.offset_coefficient * offset_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if global_step % 10000 == 0:\n",
    "            torch.save(unet.state_dict(), \"data/m40216/unet_%s.pth\"%str(global_step))\n",
    "            torch.save(style_encoder.state_dict(), \"data/m40216/style_encoder_%s.pth\"%str(global_step))\n",
    "            torch.save(content_encoder.state_dict(), \"data/m40216/content_encoder_%s.pth\"%str(global_step))\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Quit\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
