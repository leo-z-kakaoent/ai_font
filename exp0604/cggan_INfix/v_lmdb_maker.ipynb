{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a09539-8474-4e38-b818-aee93923b0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "\n",
    "import os\n",
    "import lmdb # install lmdb by \"pip install lmdb\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def checkImageIsValid(imageBin):\n",
    "    if imageBin is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        imageBuf = np.frombuffer(imageBin, dtype=np.uint8)\n",
    "        img = cv2.imdecode(imageBuf, cv2.IMREAD_COLOR)\n",
    "        # if img == None:\n",
    "        #     return False\n",
    "        imgH, imgW, imgC = img.shape[0], img.shape[1], img.shape[2]\n",
    "        if imgH * imgW * imgC == 0:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    # img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)\n",
    "    # imgH, imgW = img.shape[0], img.shape[1]\n",
    "    # if imgH * imgW == 0:\n",
    "    #     return False\n",
    "    return True\n",
    "\n",
    "def writeCache(env, cache):\n",
    "    with env.begin(write=True) as txn:\n",
    "        for k, v in cache.items():\n",
    "            try:\n",
    "                txn.put(k, v)\n",
    "            except:\n",
    "                print(k)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7196e4e6-952d-4c84-a10f-36d1b9ba0daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def korean2label(letter):\n",
    "        ch1 = (ord(letter) - ord('가'))//588\n",
    "        ch2 = ((ord(letter) - ord('가')) - (588*ch1)) // 28\n",
    "        ch3 = (ord(letter) - ord('가')) - (588*ch1) - 28*ch2\n",
    "        return ch1, ch2, ch3\n",
    "    \n",
    "def get_all_korean():\n",
    "\n",
    "    def nextKorLetterFrom(letter):\n",
    "        lastLetterInt = 15572643\n",
    "        if not letter:\n",
    "            return '가'\n",
    "        a = letter\n",
    "        b = a.encode('utf8')\n",
    "        c = int(b.hex(), 16)\n",
    "\n",
    "        if c == lastLetterInt:\n",
    "            return False\n",
    "\n",
    "        d = hex(c + 1)\n",
    "        e = bytearray.fromhex(d[2:])\n",
    "\n",
    "        flag = True\n",
    "        while flag:\n",
    "            try:\n",
    "                r = e.decode('utf-8')\n",
    "                flag = False\n",
    "            except UnicodeDecodeError:\n",
    "                c = c+1\n",
    "                d = hex(c)\n",
    "                e = bytearray.fromhex(d[2:])\n",
    "        return e.decode()\n",
    "\n",
    "    returns = []\n",
    "    flag = True\n",
    "    k = ''\n",
    "    while flag:\n",
    "        k = nextKorLetterFrom(k)\n",
    "        if k is False:\n",
    "            flag = False\n",
    "        else:\n",
    "            returns.append(k)\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91ee514-87cf-4767-8c67-4ea591987700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createDataset(outputPath, imagePathList, labelList, writerIDList, lexiconList=None, checkValid=True):\n",
    "    \"\"\"\n",
    "    Create LMDB dataset for CRNN training.\n",
    "    ARGS:\n",
    "        outputPath    : LMDB output path\n",
    "        imagePathList : list of image path\n",
    "        labelList     : list of corresponding groundtruth texts\n",
    "        lexiconList   : (optional) list of lexicon lists\n",
    "        checkValid    : if true, check the validity of every image\n",
    "    \"\"\"\n",
    "    assert(len(imagePathList) == len(labelList))\n",
    "    #import pdb;pdb.set_trace()\n",
    "    nSamples = len(imagePathList)\n",
    "    \n",
    "    env = lmdb.open(outputPath, map_size=1099511627776)\n",
    "    cache = {}\n",
    "    cnt = 1\n",
    "    # pbar = tqdm(total=len(imagePathList))\n",
    "    # import pdb;pdb.set_trace()\n",
    "    for i in range(nSamples):\n",
    "        imagePath = imagePathList[i]\n",
    "        label = labelList[i]\n",
    "        writerID = writerIDList[i]\n",
    "\n",
    "        if not os.path.exists(imagePath):\n",
    "            print('%s does not exist' % imagePath)\n",
    "            continue\n",
    "        with open(imagePath, 'rb') as f:\n",
    "            imageBin = f.read()\n",
    "        if checkValid:\n",
    "            if not checkImageIsValid(imageBin):\n",
    "                print('%s is not a valid image' % imagePath)\n",
    "                continue\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        imageKey = 'image-%09d' % cnt\n",
    "        labelKey = 'label-%09d' % cnt\n",
    "        writerIDKey = 'writerID-%09d' % cnt\n",
    "        imageKey = imageKey.encode('utf-8')\n",
    "        labelKey = labelKey.encode('utf-8')\n",
    "        writerIDKey = writerIDKey.encode('utf-8')\n",
    "        \n",
    "        cache[imageKey] = imageBin\n",
    "        cache[labelKey] = label.encode('utf-8')\n",
    "        cache[writerIDKey] = writerID.encode('utf-8')\n",
    "        # import pdb;pdb.set_trace()\n",
    "        if lexiconList:\n",
    "            lexiconKey = 'lexicon-%09d' % cnt\n",
    "            lexiconKey = lexiconKey.encode('utf-8')\n",
    "            cache[lexiconKey] = \" \".join([str(ch) for ch in lexiconList[i]]).encode('utf-8')\n",
    "        if cnt % 1000 == 0:\n",
    "            writeCache(env, cache)\n",
    "            cache = {}\n",
    "            print('Written %d / %d' % (cnt, nSamples), end=\"\\r\", flush=True)\n",
    "        cnt += 1\n",
    "        # pbar.update(1)\n",
    "    nSamples = cnt-1\n",
    "    cache['num-samples'.encode()] = str(nSamples).encode()\n",
    "    writeCache(env, cache)\n",
    "    print('Created dataset with %d samples' % nSamples)\n",
    "    # pbar.set_postfix('Created dataset with %d samples' % nSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a82059-ee9c-4199-9ffc-db58753d3007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ak = get_all_korean()\n",
    "cache = {k:korean2label(k) for k in ak}\n",
    "datafd = \"/home/jupyter/ai_font/data/train/pngs\"\n",
    "font_mapper = pd.read_pickle(\"/home/jupyter/ai_font/data/pickle/font_mapper.pickle\")\n",
    "fonts = font_mapper.index\n",
    "img_file_list = [f\"{datafd}/{f}\" for f in os.listdir(datafd) if f.endswith(\".png\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae383e5-aeaf-4d5f-8a55-2ae4742667fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2569554/2569554 [00:08<00:00, 304266.96it/s]\n"
     ]
    }
   ],
   "source": [
    "img_path_list = []\n",
    "label_list = []\n",
    "ID_list = []\n",
    "lexicon_list = []\n",
    "\n",
    "for img_path in tqdm(img_file_list):        \n",
    "    img_path_list.append(img_path)\n",
    "    label = img_path.split(\"/\")[-1].split(\"__\")[-1].replace(\".png\",\"\")\n",
    "    label_list.append(label)\n",
    "    lexicon = cache[label]\n",
    "    lexicon_list.append(lexicon)\n",
    "    writerID_str = img_path.split(\"/\")[-1].split(\"__\")[-2]\n",
    "    writerID = fonts.get_loc(writerID_str)\n",
    "    ID_list.append(writerID_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d143fc1-a0e6-4d2d-9b30-2a7313765e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample: 2569554\n",
      "Created dataset with 2569554 samples\n"
     ]
    }
   ],
   "source": [
    "print('total sample: %d' % len(img_path_list))\n",
    "\n",
    "createDataset('/home/jupyter/ai_font/data/train/lmdb', img_path_list, label_list, ID_list,lexicon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4dc749-2d45-452e-9aa1-b6a86ce37e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
